{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Symptom-tracking-roBERTa-pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMsZRGa8LXO0OSGd1oTzsoy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dilaratank/roBERTa-Symptom-Tracking/blob/main/Symptom_tracking_roBERTa_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htaV-iTR9I6E"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8__E7KiE9Ui-"
      },
      "source": [
        "## Motivation\n",
        "\n",
        "The COVID-19 pandemic is a challenging time for all of us. AI-models in NLP could help process COVID-19 information in medical interviews more interpretable and visual. This research will focus on symptom extraction from medical dialogue before and after COVID-19.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuFLc38x9V-Z"
      },
      "source": [
        "## RQs\n",
        "\n",
        "What are the most common symptoms before and after COVID-19 in medical interviews? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbQUYsZJ9XFD"
      },
      "source": [
        "## Example as a task illustration\n",
        "\n",
        "The code is designed to work as follows:\n",
        "- Feed it a medical dialogue, where symptoms are discussed\n",
        "- The code will extract the symptoms\n",
        "- The code will display the most common symptoms in that conversation\n",
        "\n",
        "An example can be illustrated with the following conversation: \\\\\n",
        "Patient: Hello doctor, these last few days I have been coughing a lot. \\\\\n",
        "Doctor: That is unfortunate to hear, do you have any other symptoms like sore throat, chest pains, etc? \\\\\n",
        "Patient: I have a cough and in addition to that also a sore throat, but that's about it. \\\\\n",
        "Doctor: Alright, a cough and a sore throat are symptoms of the Coronavirus but because of the time of year I assume you just have a cold. It is advised to take a test and take medicine for your cough and sore throat. \\\\\n",
        "\n",
        "The model would then output 'cough' and 'sore throat' as most common symtpoms discussed in this dialogue. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EgvEWto9jzO"
      },
      "source": [
        "## Related literature\n",
        "- BERT model\n",
        "- roBERTa model\n",
        "- Named Entity Recognition "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jECSpkht9mXz"
      },
      "source": [
        "# Experimental Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EPlbi6rEH78"
      },
      "source": [
        "## Imports and installations "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNeUIsMDIVw9",
        "outputId": "745cae12-8f44-41ac-84a2-434ae637421e"
      },
      "source": [
        "# Imports\n",
        "import zipfile\n",
        "import nltk\n",
        "import os\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import tokenize\n",
        "nltk.download('punkt')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Installations\n",
        "\n",
        "# To avoid version conflic on Colab notebook\n",
        "# %pip install pip -U\n",
        "# %pip install sentencepiece\n",
        "# %pip install sortedcontainers==2.1.0\n",
        "\n",
        "# # The model\n",
        "# %pip install tner"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niUX0Vln9sod"
      },
      "source": [
        "## Datasets\n",
        "\n",
        "The following datasets will be used: \n",
        "- COVID-19 Dialogue Datase (during/after covid) https://www.kaggle.com/xuehaihe/covid-dialogue-dataset?select=COVID-Dialogue-Dataset-English.txt\n",
        "- MedDialog Dataset (English) (before covid) https://github.com/UCSD-AI4H/Medical-Dialogue-System\n",
        "  - This dataset consists of 4 datasets, the 'icliniq' dataset will be used for this project because of the wide variety of medical subjects\n",
        "\n",
        "These datasets are structured as follows: \n",
        "- ID number\n",
        "- Description\n",
        "- Dialogue\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1KknLl5PgtG"
      },
      "source": [
        "### Download data\n",
        "Let's first download the data!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-Kih1o0PpDK",
        "outputId": "fd4c5b5a-68be-4f6e-8aef-0b0570330d5a"
      },
      "source": [
        "!git clone https://github.com/dilaratank/roBERTa-Symptom-Tracking.git\n",
        "%cd roBERTa-Symptom-Tracking/\n",
        "with zipfile.ZipFile(\"/content/roBERTa-Symptom-Tracking/data.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"/content/\")\n",
        "%cd /content/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'roBERTa-Symptom-Tracking'...\n",
            "remote: Enumerating objects: 21, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 21 (delta 4), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (21/21), done.\n",
            "/content/roBERTa-Symptom-Tracking/roBERTa-Symptom-Tracking\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7V5oyLkXM2mh"
      },
      "source": [
        "### Dataset Preprocessing\n",
        "\n",
        "The roBERTa model requires the data to be structured per sentence, which is done in the preprocessing steps. First, the data is split on dialogue to get rid of other unneccessary information. Thereafter, the data is split on sentences and saved as a .csv file for later use. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thrZmo1SOU9Z"
      },
      "source": [
        "# Helper functions\n",
        "\n",
        "def split_on_dialogue(data_path):\n",
        "    \"\"\"\n",
        "    Returns list with conversations\n",
        "    Format conversatoins: [[conversation1], [conversation2], ..., [conversation_n]]\n",
        "    \"\"\"\n",
        "    \n",
        "    with open(data_path) as f:\n",
        "        lines = f.readlines()\n",
        "        f.close()\n",
        "\n",
        "    i = 0\n",
        "    j = 0\n",
        "    dialogue_i = 0\n",
        "    convo=[]\n",
        "    conversations=[]\n",
        "\n",
        "    for line in lines:\n",
        "        i += 1\n",
        "        tokens = word_tokenize(line)\n",
        "\n",
        "\n",
        "        if line[:8] == 'Dialogue':\n",
        "            dialogue_i = i+1\n",
        "\n",
        "        if i == dialogue_i+j:\n",
        "            convo.append(line)\n",
        "            j +=1\n",
        "            if len(tokens) == 0:\n",
        "                conversations.append(convo)\n",
        "                convo = []\n",
        "                j = 0\n",
        "                continue\n",
        "    return conversations\n",
        "\n",
        "def split_on_sentences(conversations):\n",
        "  \"\"\"\n",
        "  A Function that splits the conversations in sentences. \n",
        "  \"\"\"\n",
        "\n",
        "  sentence_list = []\n",
        "\n",
        "  for conversation in conversations:\n",
        "      for sentences in conversation:\n",
        "          token_sen = tokenize.sent_tokenize(sentences)\n",
        "          for sentence in token_sen:\n",
        "              if sentence != 'Patient:' and sentence != 'Doctor:':\n",
        "                  sentence_list.append(sentence)\n",
        "    \n",
        "  return sentence_list\n",
        "\n",
        "\n",
        "def save(df, save_preprocessed_dataframe_path, name):\n",
        "    \"\"\"\n",
        "    Function that saves the created dataframe as a csv.\n",
        "    \"\"\"\n",
        "    \n",
        "    df.to_csv(save_preprocessed_dataframe_path+name+'.csv', index=False)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdYDBZeIOixt"
      },
      "source": [
        "# Final Function \n",
        "\n",
        "def preprocess_to_csv(data_path, save_to):\n",
        "  \"\"\"\n",
        "  A function that preprocesses the data (so that it is displayed per sentence),\n",
        "  and saves is as a .csv file for later use. \n",
        "  \"\"\"\n",
        "    \n",
        "  # Split on dialogue \n",
        "  conversations = split_on_dialogue(data_path)\n",
        "  \n",
        "  # Split on sentence\n",
        "  sentences = split_on_sentences(conversations)\n",
        "  \n",
        "  # Make dataframe\n",
        "  df_sent = pd.DataFrame(np.array(sentences), columns=['sentences'])\n",
        "  \n",
        "  # Save\n",
        "  name = os.path.basename(data_path)\n",
        "  save(df_sent, save_to, name[:-4])\n",
        "  \n",
        "  print(name, 'done')\n",
        "  "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kieB0cb3VeQh"
      },
      "source": [
        "Preprocessing the data might take a while!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPLGqZeITai7",
        "outputId": "6653bfef-b6e5-4786-fe65-d7fedd0140de"
      },
      "source": [
        "preprocess_to_csv('/content/data/COVID-Dialogue-Dataset-English.txt', '/content/data/')\n",
        "preprocess_to_csv('/content/data/icliniq_dialogue.txt', '/content/data/')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "COVID-Dialogue-Dataset-English.txt done\n",
            "icliniq_dialogue.txt done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifIr4G3WV8lg"
      },
      "source": [
        "The data now looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "DA9i4OY0V7U6",
        "outputId": "78a1dcd1-b038-41ad-e9fc-fa9ca552575a"
      },
      "source": [
        "covid_dialogue_df = pd.read_csv('/content/data/COVID-Dialogue-Dataset-English.csv')\n",
        "covid_dialogue_df.head()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hello doctor, I get a cough for the last few d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>No raise in temperature but feeling tired with...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>No contact with any Covid-19 persons.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>It has been four to five days and has drunk a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Doctors have shut the OP so do not know what t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           sentences\n",
              "0  Hello doctor, I get a cough for the last few d...\n",
              "1  No raise in temperature but feeling tired with...\n",
              "2              No contact with any Covid-19 persons.\n",
              "3  It has been four to five days and has drunk a ...\n",
              "4  Doctors have shut the OP so do not know what t..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRqwXqkQ9uDP"
      },
      "source": [
        "## Aproaches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jpV5-vuVttG"
      },
      "source": [
        "- Use pre-trained roBERTa to extract symptoms from medical dialogue before and after COVID-19\n",
        "- Display most common symtoms\n",
        "- Model evaluation using accuracy score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKPPnVTe9vYU"
      },
      "source": [
        "## Implementation details"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPW_f4BjWQk-"
      },
      "source": [
        "# TODO: explain and write functions that extract the symptoms from the code "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6ZtRnfg9xuv"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azUVXUXD9y_c"
      },
      "source": [
        "## Metrics\n",
        "The model will be evaluated using the accuracy metric. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2zp_aT7Wfhw"
      },
      "source": [
        "def get_predicted_symptoms(prediction):\n",
        "  \"\"\"\n",
        "  This function takes in the prediction of a sentence of the pre-trained model \n",
        "  and returns the symptoms mentioned in that sentence. \n",
        "  \"\"\"\n",
        "  symptoms = []\n",
        "\n",
        "  # Check if there is a predicted entity\n",
        "  if len(prediction[0]['entity']) > 0:\n",
        "\n",
        "    number_of_entities = len(prediction[0]['entity'])\n",
        "\n",
        "    # Loop over predicted entities and get symptoms (here called: disease)\n",
        "    for i in range(number_of_entities):\n",
        "      if prediction[0]['entity'][i]['type'] == 'disease':\n",
        "        symptoms.append(prediction[0]['entity'][i]['mention'])\n",
        "\n",
        "  return symptoms \n",
        "\n",
        "def accuracy(df):\n",
        "  \"\"\"\n",
        "  This function computes the accuracy score, given a dataframe. \n",
        "  \"\"\"\n",
        "\n",
        "  number_of_symptoms = 0\n",
        "  number_of_well_predicted = 0\n",
        "\n",
        "  for ind in df.index:\n",
        "\n",
        "    sentence = df['sentences'][ind]\n",
        "\n",
        "    # Padding needed because algorithm is not used to small sentences\n",
        "    if len(sentence) < 45:\n",
        "      sentence = sentence+'...'\n",
        "\n",
        "    prediction = trainer.predict([sentence])\n",
        "\n",
        "    predicted_symptom = get_predicted_symptoms(prediction)\n",
        "    predicted_symptom = [x.lower() for x in predicted_symptom]\n",
        "    predicted_symptom = [x.split(', ')[0] for x in predicted_symptom]\n",
        "\n",
        "    gt_symptom = df['symptoms'][ind]\n",
        "\n",
        "\n",
        "    # If it's not nan\n",
        "    if isinstance(gt_symptom, str):\n",
        "      gt_list = gt_symptom.split(', ')\n",
        "\n",
        "      # Keep track of symptoms \n",
        "      for symptom in gt_list:\n",
        "        number_of_symptoms += 1\n",
        "\n",
        "        # Keep track of well predicted symptoms \n",
        "        if symptom in predicted_symptom:\n",
        "          number_of_well_predicted += 1\n",
        "\n",
        "  print('Ground truth symptoms: ', number_of_symptoms)\n",
        "  print('Correctly predicted symptoms ', number_of_well_predicted)\n",
        "  print('accuracy: ', number_of_well_predicted/number_of_symptoms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41nVfyx790IY"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrZNbqoN91cM"
      },
      "source": [
        "## Error analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvVhnQFC92ig"
      },
      "source": [
        "# Findings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wljVkMex932G"
      },
      "source": [
        "## Illustration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL7o1lf195Ae"
      },
      "source": [
        "## Interpretation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueBtT_1r96Yt"
      },
      "source": [
        "## Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k6d_6GT97Sx"
      },
      "source": [
        "# Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8mw1A4O99HF"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYSuaNYJ9-Uo"
      },
      "source": [
        "## Lessons learned"
      ]
    }
  ]
}